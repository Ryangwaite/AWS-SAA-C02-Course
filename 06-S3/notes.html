<h2>S3 Basics</h2>
<h3>S3 Security</h3>
<p>Important: <strong>S3 is private by default!</strong></p>
<p>Only the account root user of the account that created the bucket
have any permissions for the account.</p>
<h4>S3 Bucket Policy</h4>
<p>A form of <strong>resource policy</strong>. They are similar to an identity policy, but
they are attached to resources instead of identities. Resource Policies provide
a resource perspective on permission.</p>
<p>With identity policies you control what that identity can access.</p>
<p>With resource policies you control who can access that resource.</p>
<p>Identity policies can only be attached to identities in your own account. You
have no way of giving an identity in another account access to a bucket.</p>
<p>Resource policies can allow or deny access from different accounts.</p>
<p>Resource policies can allow or deny anonymous principals. This is explicitly
declared in the bucket policy itself.</p>
<p>Each bucket can only have one policy, but it can have multiple statements.</p>
<h4>ACL's (Legacy)</h4>
<p>A way to apply a subresource to objects and buckets.
These are legacy and AWS does not recomment their use.
They are inflexible and allow simple permissions.</p>
<h4>Exam Power Up</h4>
<p>Identity or Bucket Access:</p>
<ul>
<li>
<p>Identity</p>
<ul>
<li>Controlling different resources. Not every service supports resource
policies.</li>
<li>IAM is the only place you can control permissions for everything.</li>
<li>If you have access to all accounts accesing the information</li>
</ul>
</li>
<li>
<p>Bucket</p>
<ul>
<li>Managing permissions on a specific product.</li>
<li>If you need anonymous or cross account access.</li>
</ul>
</li>
<li>
<p>ACL's</p>
<ul>
<li>NEVER - unless you must</li>
<li>AWS recommends against their use</li>
</ul>
</li>
</ul>
<h3>S3 Static Hosting</h3>
<p>Normal access is via AWS APIs.</p>
<p>This allows access via HTTP.</p>
<p>We need to point the index document at a specific document.</p>
<p>They need to be HTML files. When this is enabled, it creates
a <strong>website endpoint</strong>. This is influenced by the bucket name and region it is
in. This is not able to be changed.</p>
<p>You can use custom domains.</p>
<h4>Offloading</h4>
<p>Website with top 10 animals running on the compute service EC2</p>
<p>Take all of the images that the compute service hosts and move that to
an S3 bucket with static website hosting enabled.</p>
<p>When the compute service creates the HTML file and delivers to the user,
it will point to S3 instead of the compute service directly.</p>
<h4>Out-of-band pages</h4>
<p>This may be an error page to display maintenance if the server goes offline.</p>
<p>We could then change our DNS and move customers to a backup website on S3.</p>
<h4>S3 Pricing</h4>
<ul>
<li>Cost to store data, per GB / month fee</li>
<li>Data transfer fee
<ul>
<li>Data in is always free</li>
<li>Data out is a per GB charge</li>
</ul>
</li>
<li>Each operation has a cost per 1000 operations.</li>
</ul>
<p>If you are using static website hosting, you won't be transfering much data.</p>
<p>If you end with a large customer base you may end up with many requests.</p>
<h3>Object Versioning and MFA Delete</h3>
<p>Versioning is off by default. Once it is turned on, it cannot go back.</p>
<p>If you modify an object, the original of that object is replaced.</p>
<p>Versioning lets you store multiple versions of objects within a bucket.</p>
<p>Objects which would modify objects <strong>generate a new version</strong></p>
<p><strong>ID of Object</strong> - when versioning is disabled, this is set to <strong>null</strong></p>
<p>When versioning is enabled, the ID will be associated to an object.</p>
<p>The latest or current version is always returned when an object version
is not requested.</p>
<p>When an object is deleted, AWS puts a <strong>delete marker</strong> on the object
and hides all previous versions. You could delete this marker to enable
the item.</p>
<p>To delete an object, you must delete all the versions of that object
using their version marker.</p>
<h4>MFA Delete</h4>
<p>Enabled within version configuration in a bucket. This means MFA is required
to change bucket versioning state.</p>
<p>MFA is required to delete versions.</p>
<p>You pass the serial number and the code passed with API calls.</p>
<h3>S3 Performance Optimization</h3>
<h4>Single PUT Upload</h4>
<p>By default once an object is uploaded to S3, it is sent as a single stream
of data. If a stream fails, the whole upload fails. This requires a full
restart of the data transfer.</p>
<p>Single PUT upload up to 5GB</p>
<h4>Multipart Upload</h4>
<p>Data is broken up into smaller parts.
The minimum daa size is 100 MB for multipart.</p>
<p>Upload can be split into maximum of 10,000 parts.</p>
<p>The last part can be smaller than 5MB as needed.</p>
<p>Parts can fail in isolation and be restarted in isolation.</p>
<p>The risk of uploading large amounts of data is reduced.</p>
<p>Transfer rate = speed of all parts.</p>
<h4>S3 Accelerated Transfer (off)</h4>
<p>Uses the network of AWS edge locations.</p>
<p>S3 bucket needs to be enabled for transfer acceleration.</p>
<h5>Bucketname cannot contain periods and must be DNS compatable in the naming</h5>
<p>Benefits improve the larger the location and distance. The worse the start, the
better the performance benefits.</p>
<h3>Encryption 101</h3>
<h4>Different Approaches to Encryption</h4>
<h5>Encryption at Rest</h5>
<p>When data is stored on a laptop, it is encrypted and then decrypted using
a <strong>secret</strong> in this case a password.</p>
<p>If the laptop is stolen or tampered with, the data is already encrypted and
useless.</p>
<p>This is used commonly within cloud enviroments. Even if someone could
find and access the base storage device, they can't do anything with it.</p>
<ul>
<li>Only one entity involved</li>
</ul>
<h5>Encryption in Transit</h5>
<p>Apply an encryption tunnel outside the raw data. Anyone looking
from the outside will only see a stream of scrambled data.</p>
<p>This is used when there are multiple parties or systems at play.</p>
<h4>Different Concepts</h4>
<p><strong>plaintext</strong>, unencrypted data. Can be text, but can be anything including
images or applications. This is something you can read or use immedietly.</p>
<p>Algorithm takes <strong>plaintext</strong> and a <strong>key</strong> to generate <strong>ciphertext</strong>.</p>
<h4>Symmetric Encryption</h4>
<p>Difficult becuase the key needs to be transfered securely.</p>
<p>If the data is time sensitive, the key needs to be arranged beforehand.</p>
<h4>Asymmetric Encryption</h4>
<p>The <strong>public key</strong> is uploaded to cloud storage.</p>
<p>Only the <strong>private key</strong> can decrypt any of the data</p>
<p>The risk is low because the worst that happens is someone steals the key and
can encrypt data only the original <strong>public key</strong> maker created.</p>
<h4>Signing</h4>
<p>Encryption does not necessarily mean the data came from the proper party.</p>
<p>You can use the public key from one party to confirm if the private key
did sign the message that was sent or not.</p>
<h4>Steganography</h4>
<p>Encryption is obvious when used. There is no scope denying that the
data was encrypted. Someone could force you to decrypt the data packet.</p>
<p>A file can be hidden in an image or other file. If it difficult
to find the message unless you know what to look for.</p>
<p>One party would take another's public key and encrypt some data. Then take
that data and use Steganography to hide the value in the image.</p>
<h3>Key Management Service (KMS)</h3>
<p>Regional and a Public Service. Each region is isolated when using KMS.</p>
<p>It can be connected to by anything with permission in the public zone.</p>
<p>Create, store, and manage keys. Can handle both symmetric and asymmetric keys.</p>
<p>KMS can perform cryptographic operations itself.</p>
<p>Keys never leave KMS.</p>
<p>Keys use <strong>FIPS 140-2 (L2)</strong> security standard. Some of KMS's features
are compliant with Level 3, but all are Level 2.</p>
<p>KMS manages <strong>CMK - Cusstomer Master Key</strong>. It is logical and contains</p>
<ul>
<li>ID</li>
<li>Date</li>
<li>Policy</li>
<li>Description</li>
<li>State</li>
</ul>
<p>These are all backed by <strong>physical</strong> key material.</p>
<p>You can generate or import the key material.</p>
<p>CMKs can be used for up to <strong>4KB of data</strong></p>
<h4>Data Encryption Key (DEKs)</h4>
<p>Using the generate key and a customer master key, this generates a
data encryption key that can be used to encrypt data larger than 4KB in size.</p>
<p>KMS does not store the DEKs. Once it hands the keys over, it does not care.</p>
<p>When the DEK is generated, KMS provides two version.</p>
<ul>
<li>Plaintext Version - This can be used immedietly</li>
<li>Ciphertext Version - Encrypted version of the DEK. This is encrypted
by the customer master key that generated it.</li>
</ul>
<p>DEK is generated right before something is encrypted.</p>
<p>The data is encrypted with the plaintext version to generate Cyphertext.</p>
<p>After this, the plaintext version is discarded.</p>
<p>The encrypted data and the encrypted DEK are stored next to each other.</p>
<h4>Important Concepts.</h4>
<p>Customer Master Keys (CMK) are isolated to a region and never leave</p>
<p>Two types of CMKs: AWS managed or Customer managed CMKs</p>
<p>Customer managed keys are more configurable.</p>
<p>CMKs support key rotation.</p>
<ul>
<li>AWS automatically rotates the keys every 1095 days (3 years)</li>
<li>Customer managed keys rotate every year.</li>
</ul>
<p>CMK itself contains the current backing key, physical material used to encrypt
and decrypt, as well as previous backing keys.</p>
<p>You can create an alias. This is also per region.</p>
<h4>Key Policy (resource policy)</h4>
<p>Every customer managed key (CMK) has one.</p>
<p>Unlike other policies, the key must be told to trust the key.</p>
<p>In order for IAM to work, IAM is trusted by the account, and the account
must be trusted by the key.</p>
<h3>KMS Key Demo</h3>
<h4>Linux/macOS commands</h4>
<p>aws kms encrypt <br>
--key-id alias/catrobot <br>
--plaintext fileb://battleplans.txt <br>
--output text <br>
--query CiphertextBlob <br>
--profile iamadmin-general | base64 <br>
--decode &gt; not_battleplans.enc</p>
<p>aws kms decrypt <br>
--ciphertext-blob fileb://not_battleplans.enc <br>
--output text <br>
--profile iamadmin-general <br>
--query Plaintext | base64 --decode &gt; decryptedplans.txt</p>
<h3>Object Encryption</h3>
<p>Buckets aren't encrypted, <strong>objects are</strong>. Each object can use a different
encryption method. Both of the two types use encryption in transit for S3.</p>
<p>S3 object encryption is focused on encryption at rest. Encryption in
transit comes for free with S3</p>
<ul>
<li>
<p><strong>client</strong> side encryption</p>
<ul>
<li>objects being encrypted are done by the client before they leave.</li>
<li>the whole time it is sent as cypher text</li>
<li>encryption burder is on the customer and not AWS</li>
</ul>
</li>
<li>
<p><strong>server</strong> side encryption</p>
<ul>
<li>even though the data is encrypted in transit, the objects themselves
are not encrypted at the beginning of transit. When the data reaches
the S3 endpoint, it is still plaintext. Only once it has hit S3 completely,
S3 will then encrypt before storage.</li>
<li>AWS will handle some or all of these processes.</li>
</ul>
</li>
</ul>
<h4>Server Side Encryption</h4>
<p>Three general types:</p>
<h5>SSE-C (Server-side encryption with customer provided keys)</h5>
<ul>
<li>Customer is responsible for the keys themselves.</li>
<li>Offloads CPU requirements for encryption.</li>
<li>When placing an object in S3, it requires a key.</li>
<li>S3 will see the original object throughout the process.</li>
<li>Once the key and object arrive, it is encrypted. A hash of the key is
taken and attached to the object.</li>
<li>The hash can identify if the specific key was used to encrypt the object.</li>
<li>The key is then discarded after the hash is taken.</li>
</ul>
<h5>SSE-S3 AES256 (Server-side encryption w/ Amazon S3 managed keys)</h5>
<ul>
<li>
<p>When putting data into S3, only need to provide plaintext.</p>
</li>
<li>
<p>S3 generates master key. This is fully managed and rotated without
your control.</p>
</li>
<li>
<p>When an object is added, it generates a key specifically for that one
object. It uses that key to encrypt that plaintext object.</p>
</li>
<li>
<p>The master key is used to encrypt that one key.</p>
</li>
<li>
<p>The original key is discarded.</p>
</li>
<li>
<p>The encrypted key is stored next to the encrypted object.</p>
</li>
<li>
<p>Very little control how the keys are used.</p>
</li>
<li>
<p>Most of situations, this is the default type of encryption</p>
<ul>
<li>Strong algorythm</li>
<li>Data encrypted at rest</li>
<li>Little admin overhead.</li>
</ul>
</li>
<li>
<p>THREE PROBLEMS:</p>
<ul>
<li>Regulatory enviromment where the keys and access needs to be controlled.</li>
<li>No way to control key material rotation.</li>
<li>No role seperation.
<ul>
<li>A full S3 admin can rotate keys as well as encrypt or decrypt data.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5>SSE-KMS (Server-side encryption w/ customer master keys stored in AWS KMS)</h5>
<ul>
<li>Similar as above, except for the master key.</li>
<li>Customer Master Key is managed by KMS.</li>
<li>Everytime an object is uploaded, S3 uses a dedicated key to encrypt
that specific object.</li>
<li>The key is a data encryption key that KMS generates using the CMK.</li>
<li>S3 is provided with a plaintext version of the data encryption key as well as
a plaintext one.</li>
<li>The plaintext one is used to encrypt the object then it is discarded.</li>
<li>The encrypted data encrypted key is stored along with the encrypted object.</li>
</ul>
<h6>Data encryption keys more than 4KB</h6>
<p>Every object that is uploaded and encrypted with SSE-KMS requires
a CMK. The CMK is used to generate one unique data encryption key for
each object that is encrypted using SSE-KMS.</p>
<p>When uploading an object, you can create and use a customer managed CMK. This
allows the user to control the permissions and the usage of the key material.</p>
<p>In regulated industries, this is reason enough to use SSE-KMS</p>
<p>You can also add logging and see any calls against this key from cloudtrails.</p>
<p>The best benefit is the role seperation. To decrypt any object, you need
access to the CMK that was used to generate the unique key that was used to
generate them.</p>
<p>The CMK is used to decrypt the data encryption key for that object.
That decrypted data encryption key is used to decrypt the object itself.</p>
<p>If you don't have access to KMS, you don't have access to the object.</p>
<h3>S3 Object Storage Classes</h3>
<p>Picking a storage class can be done while uploading a specific object.
The default is S3 standard. Once an object is uploaded to a specific class,
it can be changed as long as the conditions are met</p>
<h4>S3 Standard</h4>
<p>The default AWS storage class that's used in S3. This is pretty good for most
cases and should be user default as well.</p>
<p>When you store an object in S3, it is stored in a bucket within a specific region. S3 is a <strong>region resillent</strong> service which means it can tolerate the
failure of an availability zone.</p>
<p>This is done by replicating objects to at least 3+ AZs when they are uploaded.</p>
<p>If an availability zone does fail, the object is safe in at least another two.</p>
<p>With S3 standard, it is never less than 3 availability zones.</p>
<p>This has 11, 9's for Object Durability and 4, 9's for availability.</p>
<p>Offers low latency and high throughput.</p>
<p>No <strong>minimums</strong> or <strong>delays</strong> or <strong>penalties</strong></p>
<p>All of the other storage classes trade some compromises for another.</p>
<h4>S3 Standard-IA</h4>
<p>Designed for less frequent, but rapid access when it is needed.</p>
<p>Cheaper rate to store data you will rarely need, but if you do need it, you
need it quickly.</p>
<p>This is approximately 54% cheaper for the base rate.</p>
<p>No matter what the size of the object, there is a minimum of 128KB min charge.</p>
<p>The cost benefits might be negated for smaller objects.</p>
<p>30 days minimum duration charge per object.</p>
<p>Also charged a retrival fee for every GB of data retrieved from this class.</p>
<p>99.9% availability, slightly lower than standard S3.</p>
<p>Designed for data that isn't accessed often, long term storage, backups,
disaster recovery files. The requirement for data to be safe is most important.</p>
<h4>One Zone-IA</h4>
<p>Designed for data that is accessed less frequently but needed quickly.</p>
<p>80% of the base cost of Standard-IA.</p>
<p>Same minimum size and duration fee.</p>
<p>Data is only stored in a single AZ, no 3+ AZ replication.</p>
<p>Great choice for secondary copies of primary data or backup copies.</p>
<p>If data is easily recreatable from a primary data set, this would be a great
place to store the output from another data set.</p>
<p>Designed for 99.5% availability.</p>
<p>This storage class cannot withstand AZ failure.</p>
<h4>S3 Glacier</h4>
<p>No immediate access to objects. Make a request to access objects then after
a duration, you get the access. Retrieval time anywhere from 1 min - 12 hrs</p>
<p>Secure, durable, and low cost storage for archival data.
Cost is 17% of S3 standard
11 9's durability and 4 9's availability, 3+ AZ replication.</p>
<p>40KB minimum object capacity charge</p>
<p>90 days minimum storage duration charge.</p>
<p>Retrieval in minutes or hours.</p>
<h5>Expedited</h5>
<p>Retrieval between 1 - 5 minutes, most expensive</p>
<h5>Standard</h5>
<p>Retrievals take 3 - 5 hours to restore.
This is good for backup data or original media if there was a mistake.</p>
<h5>Bulk retrievals</h5>
<p>Retrievals take 5 - 12 hours.
Lowest cost and is used for large amounts of data.</p>
<h4>S3 Glacier Deep Archive</h4>
<p>Designed for long term backups and as a <strong>tape-drive</strong> replacement.</p>
<p>Currently 4.3% of S3-Standard.</p>
<p>180 days minimum storage duration charge.</p>
<p>Standard Retrieval within 12 hours, bulk storage in 48 hours.</p>
<p>Cannot use to make data public or download normally.</p>
<h4>S3 Intelligent-Tiering</h4>
<p>Combination of standard and standard IA.</p>
<p>Uses automation to remove overhead of moving objects.</p>
<p>Charges $0.0025 per 1,000 objects.</p>
<p>If an object is not accessed for 30 days, it will move into infrequent access.</p>
<p>This is good for objects that are unknown their access pattern.</p>
<h3>Object Lifecycle Management</h3>
<p>Intelligent-Tiering is used for objects where access is unknown.</p>
<p>S3 has features that can move objects around automatically and purge
objects under certain circumstances.</p>
<p>A lifecycle configuration is a set of <strong>rules</strong> that consists of <strong>actions</strong>.</p>
<h4>Transition Actions</h4>
<p>Change the storage class over time.</p>
<p>An example is:</p>
<ul>
<li>Move an object from S3 to IA after 90 days</li>
<li>After 180 days move to Glacier</li>
<li>After one year move to Deep Archive</li>
</ul>
<p>Objects must flow downwards, they can't flow in the reverse direction.</p>
<h4>Expiration Actions</h4>
<p>Once an object has been uploaded and changed, you can purge older versions
after 90 days to keep costs down.</p>
<h3>S3 Replication</h3>
<p>Architecture for both is similar, only difference is if both buckets are
in the same account or different accounts.</p>
<p>An IAM Role is configured during the replciation configuration.</p>
<p>Two types of replication</p>
<h4>Cross-Region Replication (CRR)</h4>
<h4>Same-Region Replication (SRR)</h4>
<p>The role is configured to allow the S3 service to assume it based on
its trust policy. The permission policy allows it to read objects on the
source bucket and copy them to the destination bucket.</p>
<p>When different accounts are used, the role is not by default trusted
by the destination account. If configuring between accounts, you must
add a bucket policy on the destination account to allow the IAM role to
access the bucket.</p>
<h4>S3 Replication Options</h4>
<ul>
<li>All objects or a subset of those objects
<ul>
<li>Filter can be defined to make the subset.</li>
</ul>
</li>
<li>Select which storage class the destination bucket will use.
<ul>
<li>The default is the same type of storage, but this can be changed.</li>
</ul>
</li>
<li>Define the ownership of the objects.
<ul>
<li>The default is the ownership will be the same as the source account.</li>
<li>This is a problem if the buckets are in different accounts.</li>
<li>The objects in the destination bucket are not owned by that account.</li>
</ul>
</li>
<li>Replication Time Control (RTC)
<ul>
<li>Adds a guaranteed level of SLA within 15 minutes for extra cost.</li>
<li>This is useful for buckets that must be in sync the whole time.</li>
</ul>
</li>
</ul>
<h4>Important Replication Tips</h4>
<p>Replication is not retroactive. If you enable replication on a bucket
that already has objects, the old objects will not be replicated.</p>
<p>Both buckets must have versioning enabled.</p>
<p>It is a one way replication process only.</p>
<p>Replication by default can handle objects that are unencrypted or SSE-S3
With extra configuratiion it can handle SSE-KMS, but KMS requires more
configuration to work.</p>
<p>It cannot use objects with SSE-C because AWS does not have the keys
necessary.</p>
<p>Source bucket owner needs permissions to objects. If you grant cross-account
access to a bucket. It is possible the source bucket account will not own
some of those objects.</p>
<p>Will not replicate system events, glacier, or glacier deep archive.</p>
<p>No deletes are replicated.</p>
<h4>Why use replication</h4>
<p>SRR - Log Aggregation
SRR - Sync production and test accounts
SRR - Resilience with strict sovereignty requirements
CRR - Global resilience improvements
CRR - Latency reduction</p>
<h3>S3 Presigned URL</h3>
<p>A way to give another person or application access to a bucket
using your credentials in a safe way.</p>
<p>A S3 bucket without any public access configured.</p>
<p>In order to access the bucket, the IAM user must authenticate and send a
request to access those objects.</p>
<p>iamadmin can make a reqeust to S3 to <strong>generate presigned URL</strong></p>
<p>The user must provide:</p>
<ul>
<li>security credentials</li>
<li>bucket name</li>
<li>object key</li>
<li>expiry date and time</li>
<li>indicate how the object or bucket will be accessed</li>
</ul>
<p>S3 will respond with a custom URL with all the details encoded including
the expiration of the URL.</p>
<h4>Exam Powerup</h4>
<ul>
<li>You can create a URL for an object you have no access to
<ul>
<li>The object will not allow access because your user does not have it.</li>
</ul>
</li>
<li>When using the URL the permission match the identity that generated it
at the moment the item is being accessed.</li>
<li>If you get an access deny it means the ID never had access, or lost it.</li>
<li>Don't generate with a role. The role will likely expire before the url does.</li>
</ul>
<h3>S3 Select and Glacier Select</h3>
<p>This provides a ways to retrieve parts of objects and not the entire object.</p>
<p>If you retrieve a 5TB object, it takes time and consumes 5TB of data.
Filtering at the client side doesn't reduce this cost.</p>
<p>S3 and Glacier select lets you use SQL-like statement.</p>
<p>The filtering happens at the S3 bucket source</p>
